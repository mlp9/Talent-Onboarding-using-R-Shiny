{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from textblob import TextBlob, Word\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "#with open('reviewdatainonecomment.csv', 'r',encoding='utf-8') as f:\n",
    "  reader = csv.reader(f)\n",
    "  result = list(reader)\n",
    "\n",
    "result = result[0]\n",
    "result\n",
    "\n",
    "# Text cleaning \n",
    "from string import punctuation\n",
    "import re\n",
    "    clean_sentence(sentence):\n",
    "    #sentence = re.sub(r\"(?:\\@|https?\\://)\\S+|\\n+\", \"\", sentence.lower())\n",
    "    # Fix spelling errors in comments!\n",
    "    sent = TextBlob(sentence)\n",
    "    sent.correct()\n",
    "    clean = \"\"\n",
    "    for sentence in sent.sentences:    \n",
    "        words = sentence.words\n",
    "        # Remove punctuations\n",
    "        words = [''.join(c for c in s if c not in punctuation) for s in words]\n",
    "        words = [s for s in words if s]\n",
    "        clean += \" \".join(words)\n",
    "        clean += \". \"\n",
    "    return clean\n",
    "\n",
    "result = [clean_sentence(x) for x in result]\n",
    "result\n",
    "\n",
    "# Check sentiment polarity of each sentence.\n",
    "sentiment_scores = list()\n",
    "i = 0\n",
    "for sentence in result:\n",
    "    line = TextBlob(sentence)\n",
    "    sentiment_scores.append(line.sentiment.polarity)\n",
    "    if(i <= 10):\n",
    "        print(sentence + \": POLARITY=\" + str(line.sentiment.polarity))\n",
    "        i += 1\n",
    "\n",
    "\n",
    "\n",
    "sns.distplot(sentiment_scores)\n",
    "\n",
    "# Convert array of comments into a single string\n",
    "comments = TextBlob(' '.join(result))\n",
    "# Check out noun phrases, will be useful for frequent feature extraction\n",
    "comments.noun_phrases\n",
    "\n",
    "\n",
    "\n",
    "#compactness pruning:\n",
    "cleaned = list()\n",
    " phrase in comments.noun_phrases:\n",
    "    count = 0\n",
    "    for word in phrase.split():\n",
    "        # Count the number of small words and words without an English definition\n",
    "        if len(word) <= 2 or (not Word(word).definitions):\n",
    "            count += 1\n",
    "    # Only if the 'nonsensical' or short words DO NOT make up more than 40% (arbitrary) of the phrase add\n",
    "    # it to the cleaned list, effectively pruning the ones not added.\n",
    "    if count < len(phrase.split())*0.4:\n",
    "        cleaned.append(phrase)\n",
    "        \n",
    "print(\"After compactness pruning:\\nFeature Size:\")\n",
    "len(cleaned)\n",
    "\n",
    "\n",
    "\n",
    "for phrase in cleaned:    \n",
    "    match = list()\n",
    "    temp = list()\n",
    "    word_match = list()\n",
    "    for word in phrase.split():\n",
    "        # Find common words among all phrases\n",
    "        word_match = [p for p in cleaned if re.search(word, p) and p not in word_match]\n",
    "        # If the size of matched phrases set is smaller than 30% of the cleaned phrases, \n",
    "        # then consider the phrase as non-redundant.\n",
    "        if len(word_match) <= len(cleaned)*0.3 :\n",
    "            #temp.append(word)\n",
    "            #match += word_match\n",
    "            \n",
    "    phrase = ' '.join(temp)\n",
    "#     print(\"Match for \" + phrase + \": \" + str(match))\n",
    "\n",
    "    if len(match) >= len(cleaned)*0.1 :\n",
    "        # Redundant feature set, since it contains more than 10% of the number of phrases. \n",
    "        # Prune all matched features.\n",
    "        for feature in match:\n",
    "            if feature in cleaned:\n",
    "                cleaned.remove(feature)\n",
    "            \n",
    "        # Add largest length phrase as feature\n",
    "        cleaned.append(max(match, key=len))\n",
    "        \n",
    "\n",
    "print(\"After redundancy pruning:\\nFeature Size:\" + str(len(cleaned)))\n",
    "print(\"Cleaned features:\")\n",
    "cleaned\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#from nltk.corpus import stopwords\n",
    "#feature_count = dict()\n",
    "for phrase in cleaned:\n",
    "    count = 0\n",
    "    for word in phrase.split():\n",
    "         word not in stopwords.words('english'):\n",
    "            count += comments.words.count(word)\n",
    "    \n",
    "    print(phrase + \": \" + str(count))\n",
    "#    feature_count[phrase] = count\n",
    "    feature_count=feature_count[0:5]\n",
    "\n",
    "\n",
    "\n",
    "# Select frequent feature threshold as (max_count)/100 \n",
    "# This is an arbitrary decision as of now.\n",
    "counts = list(feature_count.values())\n",
    "features = list(feature_count.keys())\n",
    "threshold = len(comments.noun_phrases)/100\n",
    "#threshold=66\n",
    "\n",
    "print(\"Threshold:\" + str(threshold))\n",
    "\n",
    "frequent_features = list()\n",
    "\n",
    "for feature, count in feature_count.items():\n",
    "    count >= threshold:\n",
    "#        frequent_features.append(feature)\n",
    "print(' Features:')\n",
    "frequent_features=frequent_features[0:5]\n",
    "frequent_features\n",
    "\n",
    "\n",
    "\n",
    "def nltk_sentiment(sentence):\n",
    "    from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "    \n",
    "    nltk_sentiment = SentimentIntensityAnalyzer()\n",
    "    score = nltk_sentiment.polarity_scores(sentence)\n",
    "    return score\n",
    "#b=dataset.values.T.tolist()\n",
    "#print(b)\n",
    "nltk_results = [nltk_sentiment(row) for row in frequent_features]\n",
    "#print(nltk_results)\n",
    "results_df = pd.DataFrame(nltk_results)\n",
    "#print(results_df)\n",
    "text_df = pd.DataFrame(frequent_features)\n",
    "#print(text_df)\n",
    "nltk_df = text_df.join(results_df)\n",
    "#nltk_df1=nltk_df[[0,'neu']]\n",
    "#print(nltk_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "nltk_df.head(5)\n",
    "\n",
    "newdf=nltk_df[0]\n",
    "newdf=pd.DataFrame({'features':nltk_df[0],'pos':nltk_df['pos'],'neg':nltk_df['neg']})\n",
    "newdf.pos=newdf.pos+0.2\n",
    "newdf.neg=newdf.neg-0.2\n",
    "newdf\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "newdf\n",
    "\n",
    "#noun=['positive attitude','good job knowledge','team player','customer acquisition','good visibility','back office process','successful launch','soft skills','suitable candidates','core issues']\n",
    "#pos_l=[0.71,0.42,0.3,0.2,0.2,0.6,0.2,0.5,0.43,0]\n",
    "#neg_l=[-0.1,0,-0.300,0,-0.1,0,-0.1,-0.2,0,-0.3]\n",
    "\n",
    "pos=newdf[0:5]['pos']\n",
    "neg=newdf[0:5]['neg']\n",
    "\n",
    "# data to plot\n",
    "n_groups = 5\n",
    "positive =newdf['pos'].head(5) \n",
    "negative =newdf['neg'].head(5)\n",
    " \n",
    "# create plot\n",
    "fig, ax = plt.subplots()\n",
    "index = np.arange(n_groups)\n",
    "bar_width = 0.3\n",
    "opacity = 1\n",
    " \n",
    "rects1 = plt.bar(index, positive, bar_width,\n",
    "alpha=opacity,\n",
    "color='b',\n",
    "label='positive sentiments')\n",
    " \n",
    "rects2 = plt.bar(index + bar_width, negative, bar_width,\n",
    "alpha=opacity,\n",
    "color='r',\n",
    "label='negative sentiments')\n",
    " \n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('sentiment value')\n",
    "plt.title('Top features and its sentiment')\n",
    "plt.xticks(index + bar_width, newdf['features'].head(5))\n",
    "plt.legend()\n",
    "fig.set_size_inches(20, 15)\n",
    "#plt.show()\n",
    "\n",
    "absa_list = dict()\n",
    "print(absa_list)\n",
    "# For each frequent feature\n",
    "for f in frequent_features:\n",
    "    # For each comment\n",
    "    absa_list[f] = list()\n",
    "    for comment in result:\n",
    "        blob = TextBlob(comment)\n",
    "        # For each sentence of the comment\n",
    "        for sentence in blob.sentences:\n",
    "            # Search for frequent feature 'f'\n",
    "            q = '|'.join(f.split())\n",
    "            if re.search(r'\\w*(' + str(q) + ')\\w*', str(sentence)):\n",
    "                absa_list[f].append(sentence)\n",
    "\n",
    "\n",
    "print(\"Aspect Specific sentences:\")\n",
    "absa_list\n",
    "\n",
    "\n",
    "\n",
    "scores = list()\n",
    "absa_scores = dict()\n",
    "#for k, v in absa_list.items():\n",
    "    absa_scores[k] = list()\n",
    "    for sent in v:\n",
    "        score = sent.sentiment.polarity\n",
    "        scores.append(score)\n",
    "        scores[k].append(score)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Now that we have all the scores, let's plot them!\n",
    "# For comparison, we replot the previous global sentiment polarity plot\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True, figsize=(20, 10))\n",
    "plot1 = sns.distplot(scores, ax=ax1)\n",
    "\n",
    "ax1.set_title('Aspect wise scores')\n",
    "ax1.set_xlabel('Sentiment Polarity')\n",
    "ax1.set_ylabel('# of comments')\n",
    "\n",
    "ax2.set_title('Comment wise scores')\n",
    "ax2.set_xlabel('Sentiment Polarity')\n",
    "ax2.set_ylabel('# of comments')\n",
    "\n",
    "plot2 = sns.distplot(sentiment_scores, ax=ax2)\n",
    "\n",
    "\n",
    "\n",
    "# Create data values for stripplot and boxplot\n",
    "vals = dict()\n",
    "vals[\"aspects\"] = list()\n",
    "vals[\"scores\"] = list()\n",
    "for k, v in absa_scores.items():\n",
    "    for score in v:\n",
    "        vals[\"aspects\"].append(k)\n",
    "        vals[\"scores\"].append(score)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(30, 10))\n",
    "\n",
    "color = sns.color_palette(\"Blues\", 6)\n",
    "plt.xticks(rotation=90)\n",
    "sns.set_context(\"paper\", font_scale=3) \n",
    "sns.boxplot(x=\"aspects\", y=\"scores\", data=vals, palette=color, ax=ax1)\n",
    "\n",
    "\n",
    "\n",
    "color = sns.color_palette(\"Reds\", 10)\n",
    "fig, ax1 = plt.subplots(figsize=(30, 10))\n",
    "plt.xticks(rotation=90)\n",
    "sns.set_context(\"paper\", font_scale=5) \n",
    "sns.stripplot(y=\"aspects\", x=\"scores\",data=vals, palette=color)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
